name: 'Fantastic site'
driver: pgsql

database:
  # Any tables that are NOT mentioned here would be fully copied from DB.
  tables:
    # Get 1.5% of rows (random) from this table.
    orders: 0.015
    # Get 50% of news for the last month from the hottest categories.
    latest_news:
      get: 0.5
      table: content
      # Here you can use any valid sql operators for comparison.
      where:
        type: news
        created: ['(NOW() - INTERVAL 1 MONTH)', '>=']
        category_id: ['(5, 18)', 'IN']
    # Don't copy data from tables whose names match the regex.
    cache_:
      table_regex: '/^cache_.*/'
      get: 0
    # Get 80% of rows from this table where `data` column is empty.
    # The `export_method: row` option means that each row would be immediately
    # saved to the file during the export. See more details in the `options`
    # section of this file.
    actions:
      get: 0.8
      where:
        whatever: ['data="" OR data IS NULL', 'expression']
      export_method: row

  # Database entity - is a table that has relation with other tables
  # (by foreign key). Here we can define entities that should be exported.
  # Entity name should follow [entity_type]__[entity_bundle] or just
  # [entity_type] naming convention.
  entities:
    # Export 1% of articles. Basically in our example database all content
    # is stored in `content` table, so here we get rows with type='article'.
    content__article:
      table: content
      get: 0.01
      where:
        type: article
      # Also we need to know the name of a column that represents row id
      # (if it exists).
      fields:
        id: content_id
      # As we can see articles has relations with other tables.
      relations:
        # We should also export rows from table `category` with id=[category_id
        # value of current row from `content` table].
        category:
          where:
            id: '%category_id'
          # As there can be only 1 category selected for article we can add
          # `limit 1` to optimize search performance.
          limit: 1
        # And we want to export data from `related_content` table as well.
        related_content:
          where:
            entity_type: content
            bundle: article
            entity_id: '%content_id'
          # But `related_content` table does not store content data, it just
          # links pages with each other. So we have to add another relationship.
          relations:
            # Here we want to export rows from `content` table, but it would be
            # not enough as content is an entity (has relationships with other
            # tables). So we flag it with is_entity=1 to check content entity
            # and export it with related stuff. The content__[bundle] entity
            # should be listed in this file as well to export data as expected.
            # Here we export entities by getting data from `content` table
            # with content_id=[related_entity_id value of current row from
            # `related_content` table] and bundle=... Actually bundle may be
            # different, and we can't predict it in this case.
            # But this tool is smart enough to get a row from `content`
            # table by id first and use `type` value as bundle to call
            # content__[bundle] entity export.
            # If you know bundle right away then you can add it to `values`
            # just like we add a `type: content` here.
            pages:
              table: content
              is_entity: 1
              values:
                id: '%related_entity_id'
                type: content
              fields:
                id: content_id
                bundle: type
    # Export all books.
    content__book:
      table: content
      get: 1
      where:
        type: book
      fields:
        id: content_id
      relations:
        # We should also export data from `content_users` table where
        # content_id=[content_id value of current row from `content` table].
        book_authors:
          table: content_users
          where:
            content_id: '%content_id'
          # But `content_users` table does not store author data, it just links
          # books with authors. So we have another relationship here.
          relations:
            # Here we want to export rows from `user` table, but it would be
            # not enough as user is an entity (has relationship with another
            # table). So we flag it with is_entity=1 to check `user` entity
            # and export it with related stuff. The `user` entity should
            # be listed in this file as well.
            users:
              table: user
              is_entity: 1
              values:
                id: '%user_id'
                type: user
    # We don't want any random data from `users` table, but other entities has
    # references to this table (i.e. content author), so we have to declare it
    # as an entity with all relations to make export work as expected.
    user:
      table: users
      get: 0
      fields:
        id: uid
      relations:
        users_roles:
          where:
            uid: '%uid'

  # Here you can set anonymization rules for DB tables data.
  anonymization:
    private_files:
      table: files
      # Apply this anonymization to rows with deleted=0, status!=0
      # and uri matching regex.
      # Here you can use simple php comparison operators and regex.
      where:
        deleted: 0
        status: [0, '!=']
        uri: ['!^private://.*!', 'regex']
      # Specify fields to anonymize.
      fields:
        # You can use any Faker method to generate some value here
        # (see more at https://fakerphp.github.io/).
        # Generate a filename (ex: 'aeliyodulp.txt').
        filename:
          method: 'Faker::lexify'
          args:
            - '??????????.txt'
        # Compose uri using generated filename (ex: 'private://aeliyodulp.txt').
        uri:
          method: concat
          args:
            - 'private://'
            - '%filename'
        # Replace file mime with a simple value.
        filemime: text/plain
    # Anonymize some data from users table (affects all rows).
    users:
      fields:
        # Generate a new username in a following format: [random_word]_[user_id]
        # (ex: architecto_42).
        name:
          method: concat
          args:
            -
              method: 'Faker::word'
            - _
            - '%id'
        # Replace password hash with a simple value.
        pass: '-'
        # Compose email using username (ex: architecto_42@test.test)
        mail:
          method: concat
          args:
            - '%name'
            - '@test.test'
        # Erase additional user data.
        data: null

# Here you can set some options for DB Anonymizer. These options may be useful
# for testing and debugging.
# If you want to use defaults then it's okay to remove this section.
options:
  # DB structure export step:
  structure:
    # 1 - skip this step, 0 - don't skip.
    should_skip: 0
  # DB tables export step:
  tables:
    # 1 - skip this step, 0 - don't skip.
    should_skip: 0
    # Default percentage, is used for tables that are not listed in the
    # `database.tables` section. By default, all data from such tables should be
    # fully exported (1 is 100%).
    # This option is also applies to tables without explicit `get` parameter.
    get: 1
    # During export rows are grouped in a batch under single INSERT INTO query.
    # Here you can set the max size of the batch.
    insert_rows_max: 300
    # Following options are available here:
    # - default: all exported rows (of a single table) are added to RAM and
    # saved to the file at once to improve performance and reduce disk usage
    # (but it can cause errors due to insufficient memory in case if you have
    # super large data dumps or super low RAM available).
    # - row: each row would be immediately saved to the file during the export.
    # Just keep in mind that this option exists, but try to use default
    # behavior first.
    export_method: default
  # DB entities export step:
  entities:
    # 1 - skip this step, 0 - don't skip.
    should_skip: 0
    # Default percentage, is used for entities without explicit `get` parameter.
    # By default, 1% of entities is dumped.
    get: 0.01
    # Same as `options.tables.insert_rows_max`.
    # Keep in mind that rows of related tables are normally exported by 1 in
    # a batch. However, sometimes still there are many rows in a batch and then
    # this limit makes sense.
    insert_rows_max: 300
    # Same as `options.tables.export_method`.
    export_method: default
  # DB data anonymization (works for both tables and entities):
  anonymization:
    # 1 - skip anonymization, 0 - don't skip.
    should_skip: 0
